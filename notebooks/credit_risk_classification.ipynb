{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Credit Risk Classification (Risque)\n",
        "\n",
        "**Objectif**: Construire un modèle de classification pour prédire la variable cible `Risque ∈ {Risque Elevé, Risque Faible}` à partir de 7 variables d’entrée `A1..A7`.\n",
        "\n",
        "**Livrables couverts dans ce notebook**:\n",
        "- Récupération et vérification des données\n",
        "- Exploration des données\n",
        "- Nettoyage et préparation\n",
        "- Développement d’au moins 3 modèles\n",
        "- Évaluation et comparaison\n",
        "- Sauvegarde du meilleur modèle\n",
        "\n",
        "> Dataset utilisé: `data/Risque_data.xlsx` (690 lignes, 8 colonnes).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    roc_auc_score,\n",
        ")\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
        "\n",
        "DATA_PATH = Path('../data/Risque_data.xlsx')\n",
        "# Same path/name as the Streamlit app expects\n",
        "ARTIFACT_PATH = Path('../artifacts/credit_risk_model.joblib')\n",
        "TARGET_COL = 'Risque'\n",
        "\n",
        "pd.set_option('display.max_columns', 200)\n",
        "np.random.seed(42)\n",
        "print('DATA_PATH:', DATA_PATH.resolve())\n",
        "print('ARTIFACT_PATH:', ARTIFACT_PATH.resolve())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Récupération et vérification des données\n",
        "\n",
        "df = pd.read_excel(DATA_PATH)\n",
        "print('shape:', df.shape)\n",
        "print('columns:', df.columns.tolist())\n",
        "\n",
        "# Vérifications attendues\n",
        "expected_cols = ['A1','A2','A3','A4','A5','A6','A7', TARGET_COL]\n",
        "missing = [c for c in expected_cols if c not in df.columns]\n",
        "assert not missing, f\"Colonnes manquantes: {missing}\"\n",
        "\n",
        "print('\\nDtypes:')\n",
        "display(df.dtypes)\n",
        "\n",
        "print('\\nValeurs de la cible:')\n",
        "display(df[TARGET_COL].value_counts(dropna=False))\n",
        "\n",
        "print('\\nValeurs manquantes (par colonne):')\n",
        "display(df.isna().sum())\n",
        "\n",
        "print('\\nAperçu:')\n",
        "display(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Exploration des données (EDA)\n",
        "\n",
        "On explore:\n",
        "- statistiques descriptives (numériques / catégorielles)\n",
        "- duplications\n",
        "- distribution de la cible\n",
        "- cohérence des catégories\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_cols = ['A1','A2','A3','A4']\n",
        "cat_cols = ['A5','A6','A7']\n",
        "\n",
        "print('Duplicates:', int(df.duplicated().sum()))\n",
        "\n",
        "print('\\nDescribe (numériques):')\n",
        "display(df[numeric_cols].describe().T)\n",
        "\n",
        "print('\\nDescribe (catégorielles):')\n",
        "display(df[cat_cols].describe().T)\n",
        "\n",
        "print('\\nCardinalité catégories:')\n",
        "display(pd.DataFrame({c: [df[c].nunique(dropna=True)] for c in cat_cols}).T.rename(columns={0:'n_unique'}))\n",
        "\n",
        "print('\\nQuelques valeurs par variable catégorielle:')\n",
        "for c in cat_cols:\n",
        "    vals = df[c].dropna().astype(str).unique().tolist()[:20]\n",
        "    print(f\"{c}: {vals}{' ...' if df[c].nunique(dropna=True)>20 else ''}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Nettoyage et préparation des données\n",
        "\n",
        "On applique un nettoyage simple et reproductible:\n",
        "- suppression des doublons\n",
        "- gestion des valeurs manquantes (imputation)\n",
        "- (optionnel) filtrage d’outliers sur les variables numériques\n",
        "\n",
        "Ensuite, on prépare les données pour le Machine Learning via une **pipeline sklearn**:\n",
        "- imputation + standardisation pour les variables numériques\n",
        "- imputation + one-hot encoding pour les variables catégorielles\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic cleaning (mirrors the Streamlit app defaults)\n",
        "\n",
        "raw = df.copy()\n",
        "\n",
        "# Drop duplicates\n",
        "n_dup = int(raw.duplicated().sum())\n",
        "raw = raw.drop_duplicates()\n",
        "print('duplicates removed:', n_dup)\n",
        "\n",
        "# Optional outlier filtering (disabled by default)\n",
        "USE_OUTLIER_FILTER = False\n",
        "Z_THRESHOLD = 3.0\n",
        "\n",
        "\n",
        "def zscore_keep_mask(s: pd.Series, thr: float = 3.0) -> pd.Series:\n",
        "    x = pd.to_numeric(s, errors='coerce')\n",
        "    mu = x.mean()\n",
        "    sigma = x.std(ddof=0)\n",
        "    if not np.isfinite(sigma) or sigma == 0:\n",
        "        return pd.Series([True] * len(s), index=s.index)\n",
        "    z = (x - mu) / sigma\n",
        "    return z.abs() <= thr\n",
        "\n",
        "\n",
        "clean = raw.copy()\n",
        "if USE_OUTLIER_FILTER:\n",
        "    mask = pd.Series([True] * len(clean), index=clean.index)\n",
        "    for c in numeric_cols:\n",
        "        mask &= zscore_keep_mask(clean[c], Z_THRESHOLD)\n",
        "    clean = clean.loc[mask].copy()\n",
        "\n",
        "print('rows before:', len(df), 'rows after cleaning:', len(clean))\n",
        "\n",
        "# Target check\n",
        "assert TARGET_COL in clean.columns\n",
        "assert set(clean[TARGET_COL].dropna().unique()) <= {'Risque Elevé', 'Risque Faible'}\n",
        "\n",
        "clean.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode target to binary (same convention as the app)\n",
        "# Risque Elevé -> 0, Risque Faible -> 1\n",
        "\n",
        "y = clean[TARGET_COL].map({'Risque Elevé': 0, 'Risque Faible': 1}).astype('int64')\n",
        "X = clean.drop(columns=[TARGET_COL])\n",
        "\n",
        "print('X shape:', X.shape)\n",
        "print('y distribution:')\n",
        "display(y.value_counts().rename(index={0:'Risque Elevé', 1:'Risque Faible'}))\n",
        "\n",
        "# Preprocessor\n",
        "numeric_features = numeric_cols\n",
        "categorical_features = cat_cols\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\n",
        "            'num',\n",
        "            Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='median')),\n",
        "                ('scaler', StandardScaler()),\n",
        "            ]),\n",
        "            numeric_features,\n",
        "        ),\n",
        "        (\n",
        "            'cat',\n",
        "            Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "                # Keep dense output so HistGradientBoostingClassifier can train.\n",
        "                ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
        "            ]),\n",
        "            categorical_features,\n",
        "        ),\n",
        "    ],\n",
        "    remainder='drop',\n",
        ")\n",
        "\n",
        "preprocessor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Développement des modèles (au moins 3 algorithmes)\n",
        "\n",
        "On entraîne 3 classifieurs via des pipelines complètes:\n",
        "- Régression Logistique (baseline)\n",
        "- Random Forest\n",
        "- Gradient Boosting\n",
        "\n",
        "On évalue par validation croisée stratifiée, puis on fait un test final sur un holdout.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_pipeline(model) -> Pipeline:\n",
        "    return Pipeline(steps=[('pre', preprocessor), ('clf', model)])\n",
        "\n",
        "models = {\n",
        "    'HistGB': HistGradientBoostingClassifier(random_state=42),\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=400, random_state=42),\n",
        "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
        "}\n",
        "\n",
        "scoring = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'precision': 'precision',\n",
        "    'recall': 'recall',\n",
        "    'f1': 'f1',\n",
        "    'roc_auc': 'roc_auc',\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "cv_rows = []\n",
        "cv_details = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    pipe = make_pipeline(model)\n",
        "    out = cross_validate(pipe, X, y, cv=cv, scoring=scoring, n_jobs=None)\n",
        "    row = {\n",
        "        'model': name,\n",
        "        **{f\"cv_{k}_mean\": float(np.mean(out[f\"test_{k}\"])) for k in scoring.keys()},\n",
        "        **{f\"cv_{k}_std\": float(np.std(out[f\"test_{k}\"])) for k in scoring.keys()},\n",
        "    }\n",
        "    cv_rows.append(row)\n",
        "    cv_details[name] = out\n",
        "\n",
        "cv_results = pd.DataFrame(cv_rows).sort_values('cv_f1_mean', ascending=False).reset_index(drop=True)\n",
        "cv_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose best model by CV F1 (you can change the criterion if needed)\n",
        "best_name = str(cv_results.iloc[0]['model'])\n",
        "print('Best by CV F1:', best_name)\n",
        "\n",
        "best_model = models[best_name]\n",
        "best_pipe = make_pipeline(best_model)\n",
        "\n",
        "# Final evaluation on a holdout split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.30,\n",
        "    random_state=42,\n",
        "    stratify=y,\n",
        ")\n",
        "\n",
        "best_pipe.fit(X_train, y_train)\n",
        "y_pred = best_pipe.predict(X_test)\n",
        "\n",
        "metrics = {\n",
        "    'model': best_name,\n",
        "    'accuracy': float(accuracy_score(y_test, y_pred)),\n",
        "    'precision': float(precision_score(y_test, y_pred, zero_division=0)),\n",
        "    'recall': float(recall_score(y_test, y_pred, zero_division=0)),\n",
        "    'f1': float(f1_score(y_test, y_pred, zero_division=0)),\n",
        "}\n",
        "\n",
        "# Optional AUC if available\n",
        "roc_auc = None\n",
        "if hasattr(best_pipe, 'predict_proba'):\n",
        "    try:\n",
        "        proba = best_pipe.predict_proba(X_test)[:, 1]\n",
        "        roc_auc = float(roc_auc_score(y_test, proba))\n",
        "        metrics['roc_auc'] = roc_auc\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print('Holdout metrics:')\n",
        "display(pd.DataFrame([metrics]))\n",
        "\n",
        "print('\\nConfusion matrix (rows=true, cols=pred) [0=Elevé, 1=Faible]:')\n",
        "display(pd.DataFrame(confusion_matrix(y_test, y_pred), index=['true_0','true_1'], columns=['pred_0','pred_1']))\n",
        "\n",
        "print('\\nClassification report:')\n",
        "print(classification_report(y_test, y_pred, target_names=['Risque Elevé','Risque Faible'], zero_division=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Sauvegarde du meilleur modèle\n",
        "\n",
        "On sauvegarde un bundle `joblib` qui contient:\n",
        "- `pipeline`: la pipeline sklearn complète (prétraitement + modèle)\n",
        "- `metadata`: infos utiles (colonnes, métriques, configuration)\n",
        "\n",
        "Ce format est compatible avec l’application Streamlit (chargement via `joblib.load`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save artifact (same layout as the Streamlit app expects)\n",
        "\n",
        "# Numeric ranges (for sanity checks in the app)\n",
        "numeric_ranges = {}\n",
        "for c in numeric_cols:\n",
        "    s = pd.to_numeric(clean[c], errors='coerce')\n",
        "    numeric_ranges[c] = {'min': float(np.nanmin(s)), 'max': float(np.nanmax(s))}\n",
        "\n",
        "metadata = {\n",
        "    'model_name': best_name,\n",
        "    'target_col': TARGET_COL,\n",
        "    'label_mapping': {'Risque Elevé': 0, 'Risque Faible': 1},\n",
        "    'numeric_cols': numeric_cols,\n",
        "    'categorical_cols': cat_cols,\n",
        "    'cv_results': cv_results.to_dict(orient='records'),\n",
        "    'holdout_metrics': metrics,\n",
        "    'metrics': {'numeric_ranges': numeric_ranges},\n",
        "    'cleaning': {\n",
        "        'drop_duplicates': True,\n",
        "        'use_outlier_filter': USE_OUTLIER_FILTER,\n",
        "        'z_threshold': Z_THRESHOLD,\n",
        "    },\n",
        "}\n",
        "\n",
        "ARTIFACT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "joblib.dump({'pipeline': best_pipe, 'metadata': metadata}, ARTIFACT_PATH)\n",
        "\n",
        "print('Saved artifact to:', ARTIFACT_PATH.resolve())\n",
        "print('Bundle keys:', joblib.load(ARTIFACT_PATH).keys())\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
